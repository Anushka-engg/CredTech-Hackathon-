# -*- coding: utf-8c -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W45qsE37Q_ZVUvgssn8TELBcsQlCcqlh
"""

pip install yfinance feedparser vaderSentiment shap scikit-learn xgboost plotly fredapi pandas_datareader

import os, datetime as dt, numpy as np, pandas as pd
import yfinance as yf
import feedparser
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score, classification_report
from sklearn.ensemble import RandomForestClassifier
import shap, warnings; warnings.filterwarnings("ignore")
from pandas_datareader import data as pdr
TICKERS = ["AAPL","MSFT","TSLA"]
START   = "2020-01-01"
END     = dt.date.today().isoformat()
LOOKBACK_DAYS = 180
HORIZON_DAYS  = 30
NEG_THRESH    = -0.10
analyzer = SentimentIntensityAnalyzer()

def get_ohlc(ticker):
    df = yf.download(ticker, start=START, end=END, auto_adjust=True, progress=False)
    df = df.rename_axis("date").reset_index()
    df["ticker"] = ticker
    return df

prices = pd.concat([get_ohlc(t) for t in TICKERS], ignore_index=True)
prices.head()

try:
    fred = pdr.DataReader("DGS10", "fred", START, END).rename(columns={"DGS10":"DGS10"})
    fred = fred.interpolate().reset_index().rename(columns={"DATE":"date"})
    macro = fred
except Exception:
    spx = yf.download("^GSPC", start=START, end=END, auto_adjust=True, progress=False)
    spx = spx.rename_axis("date").reset_index()[["date","Close"]].rename(columns={"Close":"SPX"}).copy()
    macro = spx

macro.head()

EVENT_KEYWORDS = {
    "downgrade": ["downgrade","cut rating","lowered outlook"],
    "restructuring": ["restructuring","refinancing","debt exchange","distressed"],
    "bankruptcy": ["bankruptcy","chapter 11","insolvency","default"],
    "probe": ["probe","investigation","fraud","sec charges"],
}

def news_features(ticker, days=LOOKBACK_DAYS):
    # Simple RSS (Yahoo Finance ticker news)
    rss = f"https://feeds.finance.yahoo.com/rss/2.0/headline?s={ticker}&region=US&lang=en-US"
    feed = feedparser.parse(rss)
    rows = []
    for e in feed.entries:
        d = pd.to_datetime(getattr(e, "published", getattr(e, "updated", dt.datetime.utcnow())))
        title = e.title if hasattr(e, "title") else ""
        sent = analyzer.polarity_scores(title)["compound"]
        flags = {k:int(any(w in title.lower() for w in v)) for k,v in EVENT_KEYWORDS.items()}
        rows.append({"date": pd.to_datetime(d.date()), "title": title, "sent": sent, **flags})
    if not rows:
        return pd.DataFrame(columns=["date","sent",*EVENT_KEYWORDS.keys()])
    df = pd.DataFrame(rows).sort_values("date")
    cutoff = pd.Timestamp.today().normalize() - pd.Timedelta(days=days)
    df = df[df["date"]>=cutoff]
    aggs = df.groupby("date").agg({"sent":"mean", **{k:"max" for k in EVENT_KEYWORDS.keys()}}).reset_index()
    aggs["ticker"] = ticker
    return aggs

news = pd.concat([news_features(t) for t in TICKERS], ignore_index=True)
news.head()

def build_features(df_px, df_news, df_macro):
    out = []
    for t, g in df_px.groupby("ticker"):
        g = g.sort_values("date").copy()

        # Make sure only numeric series for Close
        close = g["Close"].astype(float)

        g["ret"] = close.pct_change()
        g["vol_30"] = g["ret"].rolling(30).std()
        g["ret_30"] = close.pct_change(30)
        g["dd"] = (close / close.cummax() - 1.0)
        g["dd_60min"] = g["dd"].rolling(60).min()
        g["mom_20"] = close.pct_change(20)
        g["mom_60"] = close.pct_change(60)

        # Merge news
        n = df_news[df_news["ticker"] == t][["date", "sent", *EVENT_KEYWORDS.keys()]]
        g = g.merge(n, on="date", how="left")
        g["sent"] = g["sent"].fillna(0)
        for k in EVENT_KEYWORDS.keys():
            g[k] = g[k].fillna(0)

        out.append(g)

    feats = pd.concat(out, ignore_index=True)

    # Merge macro (forward-fill)
    feats = feats.merge(df_macro, on="date", how="left").sort_values(["ticker", "date"])
    feats = feats.groupby("ticker").apply(lambda x: x.ffill().bfill()).reset_index(drop=True)
    return feats

def build_features(df_px, df_news, df_macro):
    out = []
    for t, g in df_px.groupby("ticker"):
        g = g.sort_values("date").copy()

        # Ensure Close is a Series (not multiple columns)
        close = g["Close"]
        if isinstance(close, pd.DataFrame):
            close = close.iloc[:, 0]
        close = close.astype(float)

        # Calculate features
        g["ret"] = close.pct_change()
        g["vol_30"] = g["ret"].rolling(30).std()
        g["ret_30"] = close.pct_change(30)
        g["dd"] = (close / close.cummax() - 1.0)
        g["dd_60min"] = g["dd"].rolling(60).min()
        g["mom_20"] = close.pct_change(20)
        g["mom_60"] = close.pct_change(60)

        # Merge news
        n = df_news[df_news["ticker"] == t][["date", "sent", *EVENT_KEYWORDS.keys()]]
        g = g.merge(n, on="date", how="left")
        g["sent"] = g["sent"].fillna(0)
        for k in EVENT_KEYWORDS.keys():
            g[k] = g[k].fillna(0)

        out.append(g)

    feats = pd.concat(out, ignore_index=True)

    # Merge macro and fill missing
    feats = feats.merge(df_macro, on="date", how="left").sort_values(["ticker", "date"])
    feats = feats.groupby("ticker").apply(lambda x: x.ffill().bfill()).reset_index(drop=True)
    return feats

if isinstance(prices.columns, pd.MultiIndex):
    prices.columns = [c[0] if isinstance(c, tuple) else c for c in prices.columns]

import pandas as pd
import numpy as np

# --- Example EVENT_KEYWORDS ---
EVENT_KEYWORDS = {"merger":0, "acquisition":0, "scandal":0, "fraud":0}

def build_features(prices: pd.DataFrame, news: pd.DataFrame) -> pd.DataFrame:
    """
    Build clean feature set from prices & news data.
    - prices: DataFrame with columns ["date","close","ticker"]
    - news:   DataFrame with columns ["date","sent", <EVENT_KEYWORDS>, "ticker"]
    """
    feats = []

    for t in prices["ticker"].unique():
        dfp = prices[prices["ticker"]==t].copy()
        dfn = news[news["ticker"]==t].copy() if not news.empty else pd.DataFrame()

        # --- Ensure datetime ---
        dfp["date"] = pd.to_datetime(dfp["date"])
        dfp = dfp.sort_values("date").reset_index(drop=True)

        # --- Compute returns ---
        dfp["ret_1"]  = dfp["close"].pct_change()
        dfp["ret_30"] = dfp["close"].pct_change(30)   # 30-day return

        # --- Rolling volatility ---
        dfp["vol_30"] = dfp["ret_1"].rolling(30).std()

        # --- Momentum indicators ---
        dfp["mom_20"] = dfp["close"].pct_change(20)
        dfp["mom_60"] = dfp["close"].pct_change(60)

        # --- Drawdown over 60 days ---
        dfp["roll_max"] = dfp["close"].rolling(60, min_periods=1).max()
        dfp["dd_60min"] = (dfp["close"]/dfp["roll_max"] - 1).fillna(0)
        dfp.drop(columns=["roll_max"], inplace=True)

        # --- Add ticker info ---
        dfp["ticker"] = t

        # --- Merge with news sentiment & events ---
        if not dfn.empty:
            dfn["date"] = pd.to_datetime(dfn["date"])
            dfn = dfn.groupby("date").mean(numeric_only=True).reset_index()
            dfp = pd.merge(dfp, dfn, on="date", how="left")

        else:
            # If no news, fill with zeros
            dfp["sent"] = 0
            for ev in EVENT_KEYWORDS.keys():
                dfp[ev] = 0

        feats.append(dfp)

    # --- Final feature dataset ---
    feats = pd.concat(feats, ignore_index=True)

    # --- Handle missing ---
    feats = feats.fillna(0)

    return feats

import pandas as pd
import numpy as np

def build_features(prices: pd.DataFrame, news: pd.DataFrame = None) -> pd.DataFrame:
    """
    Feature engineering for stock data.
    prices -> must contain ["Date","Close","Volume","Ticker"]
    news   -> optional, must contain ["date","sent","ticker"]
    """
    g = prices.copy()

    # --- Returns ---
    g["ret"] = g["Close"].pct_change()

    # --- Rolling statistics ---
    g["ret_5d_mean"] = g["ret"].rolling(window=5).mean()
    g["ret_5d_vol"]  = g["ret"].rolling(window=5).std()
    g["ret_21d_mean"] = g["ret"].rolling(window=21).mean()
    g["ret_21d_vol"]  = g["ret"].rolling(window=21).std()

    # --- Price ratios (momentum) ---
    g["close_5d_ratio"] = g["Close"] / g["Close"].shift(5)
    g["close_21d_ratio"] = g["Close"] / g["Close"].shift(21)

    # --- Volume features ---
    g["vol_5d_mean"] = g["Volume"].rolling(window=5).mean()
    g["vol_21d_mean"] = g["Volume"].rolling(window=21).mean()

    # --- News merge (if provided) ---
    if news is not None and not news.empty:
        news = news.copy()
        news["date"] = pd.to_datetime(news["date"])
        g["Date"] = pd.to_datetime(g["Date"])

        # aggregate news sentiment per day & ticker
        news_agg = news.groupby(["date","ticker"], as_index=False)["sent"].mean()
        g = g.merge(news_agg, left_on=["Date","Ticker"], right_on=["date","ticker"], how="left")
        g.drop(columns=["date","ticker"], inplace=True)

    # --- Replace inf and handle NA ---
    g.replace([np.inf, -np.inf], np.nan, inplace=True)
    g.dropna(inplace=True)

    return g

import pandas as pd
import numpy as np

# ------------------------
# CONFIG
# ------------------------
HORIZON_DAYS = 5
NEG_THRESH   = -0.05
EVENT_KEYWORDS = {
    "bankruptcy": ["bankrupt","chapter 11"],
    "fraud": ["fraud","scandal"],
    "merger": ["merger","acquisition"],
}

# ------------------------
# FEATURE BUILDER
# ------------------------
def build_features(prices: pd.DataFrame, news: pd.DataFrame) -> pd.DataFrame:
    df = prices.copy()

    # --- Returns ---
    df["ret_30"] = df["Close"].pct_change(30)

    # --- Volatility ---
    df["vol_30"] = df["Close"].pct_change().rolling(30).std()

    # --- Momentum ---
    df["mom_20"] = df["Close"].pct_change(20)
    df["mom_60"] = df["Close"].pct_change(60)

    # --- Drawdown (last 60 days) ---
    roll_max = df["Close"].rolling(60, min_periods=1).max()
    df["dd_60min"] = df["Close"]/roll_max - 1

    # --- Merge news sentiment ---
    if not news.empty:
        # Ensure date column exists
        news["date"] = pd.to_datetime(news["date"])
        df = df.merge(news, how="left", left_on="Date", right_on="date")
        df = df.drop(columns=["date"])
    else:
        # Add empty news-related features if no news
        df["sent"] = 0.0
        for k in EVENT_KEYWORDS:
            df[k] = 0

    # Fill missing after merge
    df = df.fillna(0)

    return df

# ------------------------
# LABEL BUILDER
# ------------------------
def label_future_drop(df: pd.DataFrame, horizon: int, thresh: float) -> pd.DataFrame:
    future_ret = df["Close"].pct_change(horizon).shift(-horizon)
    df["y"] = (future_ret < thresh).astype(int)
    return df

# ------------------------
# EXAMPLE USAGE
# ------------------------
# Simulated input
prices = pd.DataFrame({
    "Date": pd.date_range("2024-01-01", periods=100, freq="D"),
    "Close": np.linspace(100,120,100) + np.random.randn(100)*2
})

news = pd.DataFrame({
    "date": pd.date_range("2024-01-01", periods=10, freq="10D"),
    "sent": np.random.uniform(-1,1,10),
    "bankruptcy": np.random.randint(0,2,10),
    "fraud": np.random.randint(0,2,10),
    "merger": np.random.randint(0,2,10),
    "ticker": ["AAPL"]*10
})

# --- Build features + labels ---
feats = build_features(prices, news)
data  = label_future_drop(feats, HORIZON_DAYS, NEG_THRESH)

print(data.head(15))

import pandas as pd

# Assuming df is your features DataFrame with labels already added
output_file = "features_with_labels.csv"
df.to_csv(output_file, index=False)

print(f"✅ Features with labels saved to {output_file}")
print("Shape:", df.shape)
print("Columns:", df.columns.tolist())

print(df.columns)
print(df.head())

stock_features = df.copy()

X = df.drop(columns=["credit_score"])
y = df["credit_score"]

X = df.drop(columns=["daily_return"])
y = df["daily_return"]

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

import joblib
joblib.dump(model, "risk_model.pkl")

# --- Task 9: Train-Test Split + Model Training + Backtest ---

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score

# ✅ Make sure features + labels exist
feature_cols = ["vol_30","ret_30","dd_60min","mom_20","mom_60","sent", *EVENT_KEYWORDS.keys()]
X = data[feature_cols]
y = data["y"]

# --- Train-test split ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, shuffle=False
)

# --- Model ---
model = RandomForestClassifier(
    n_estimators=200,
    max_depth=5,
    random_state=42,
    class_weight="balanced"
)
model.fit(X_train, y_train)

# --- Predictions ---
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# --- Evaluation ---
print("Classification Report:\n", classification_report(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, y_proba))

# --- Feature importance ---
import matplotlib.pyplot as plt

feat_imp = pd.Series(model.feature_importances_, index=feature_cols).sort_values(ascending=False)
feat_imp.plot(kind="bar", figsize=(10,4), title="Feature Importance")
plt.show()

# --- Store results ---
results = X_test.copy()
results["y_true"] = y_test.values
results["y_pred"] = y_pred
results["y_proba"] = y_proba

import pandas as pd
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
# Example: df = pd.read_csv("your_file.csv")

# Check columns
print(df.columns)

# Plot daily returns over time for a specific ticker, e.g., AAPL
ticker = "AAPL"
df_ticker = df[df["ticker"] == ticker]

plt.figure(figsize=(10, 5))
plt.plot(df_ticker["date"], df_ticker["daily_return"], marker="o", label="Daily Return")
plt.title(f"Daily Returns for {ticker}")
plt.xlabel("Date")
plt.ylabel("Daily Return")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Correlation Heatmap
plt.figure(figsize=(8,6))
sns.heatmap(df[['daily_return','volatility','news_sentiment','gdp_growth','credit_score']].corr(),
            annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

# Pairplot (optional - gives scatterplots between features)
sns.pairplot(df[['daily_return','volatility','news_sentiment','gdp_growth','credit_score']])
plt.show()

















